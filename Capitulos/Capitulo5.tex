
%---------------------------------------------------------------------
%
%                          Capítulo 5
%
%---------------------------------------------------------------------

\chapter{Desarrollo Del Proyecto}



\label{cap5:sec:Desarrollo Del Proyecto}
Una vez elegida la tecnología y las plataformas que se van a utilizar, se empieza a preparar el desarrollo del proyecto. En primer lugar se investiga que librerías y paquetes se necesitan para tener una buena conexión con \texttt{Kinect} y, además, que información obtenida del sensor de movimiento podemos aprovechar para el proyecto.


%-------------------------------------------------------------------
\section{Paquetes para Unity}

\label{cap5:sec:Paquetes para Unity}
El primer paquete que se prueba es el que nos ofrece \texttt{Microsoft} \footnote{https://developer.microsoft.com/es-es/windows/kinect/tools},que es un paquete destinado principalmente para \texttt{UnityPro}.\footnote{Unity profesional de pago con mas funcionalidad que Unity personal.} Al añadir este primer paquete al proyecto se empieza a manifestar una serie de errores de scripts, esto es debido, a la incongruencia de versiones de Unity. El proyecto se desarrolla en Unity Personal, que es una versión gratuita de Unity , mientras que el paquete que ofrece Microsoft esta especificado para Unity Pro. Aun así, se corrigen errores de comandos y llamadas a funciones obsoletas para ver si se puede aprovechar este paquete.

En una primera instancia se ejecuta la escena que viene como ejemplo en el paquete de Unity para ver su funcionamiento y se observa que tiene un comportamiento intermitente, es decir, cuando nos colocamos enfrente de la cámara de kinect a veces mostraba un esqueleto verde que emulaba la persona captada y otras veces dejaba de funcionar sin saber el error producido.
Debatiendo e intentando comprender si estos errores se producían por conexión de kinect o por funcionamiento incorrecto de la librería que nos ofrecía microsoft se opto por la opción de descartar este paquete para evitar futuras fustraciones.

Después de descartar el paquete que nos ofrecía Microsoft decidimos buscar en el Assets Store de Unity(Explicacion o referencia).Encontramos el paquete \texttt {Kinect v2 Examples with MS-SDK}\textcolor{red}{(?Referencia o imagen?)} que lo elegimos por su valoración positiva y también porque hay poca variedad de paquetes relacionados con kinect v2.

Este paquete tiene todo lo necesario para reconocer que la kinect está conectada y para poder utilizar los datos que se obtienen del sensor. En una primera toma, el paquete nos ofrece una serie de ejemplos sencillos para poder comprender mejor el funcionamiento de kinect. Para que este paquete funcione es necesario tener instalado Kinect SDK 2.0 que son los drivers de la kinect v2 .


Los ejemplos que tiene implementado el paquete de Kinect v2 Examples with MS-SDK  son variados:
\begin{itemize}
	\item AvatarsDemo, simulador que muestra un avatar en tercera persona que correspondería a la persona captada y se puede controlar sobre el escenario 3D.
	\item BackgroundRemovalDemo, son ejemplos que cambian el fondo que se encuentra detrás del usuario captado.
	\item ColliderDemo, una serie de ejemplos para ver el funcionamiento de colisiones del usuario captado con los objetos que aparecen en la escena.
	\item FaceTrackingDemo, este ejemplo reconoce la dirección de tu cabeza para girar la cámara de la imagen para simular la vista humana.
	\item FittingRoomDemo, este ejemplo te da la opción de ponerte ropa encima de tu imagen real captada.
	\item GesturesDemo, serie de ejemplos de funcionamiento de los gestos de kinect.
	\item InteractionDemo, este ejemplo muestra como el usuario puede girar,rotar y agrandar un objeto con el movimiento de sus manos.
	\item KinectDataServer, implementa un servidor de datos para guardar información como gestos de kinect.
	\item MovieSequenceDemo, este ejemplo mostrará como reproducir un conjunto de frames de película con el cuerpo del usuario.
	\item MultiSceneDemo, este ejemplo concatenará diferentes escenas de Unity basadas en las componentes de este paquete.
	\item OverlayDemo, son tres ejemplos que muestras como interactuar con los objetos de la escena, para ello, se basa en el movimientos de los brazos y manos para hacer que los objetos se mueven, roten y se desplazen.
	\item PhysicsDemo, muestra un simulación de físicas que capta el movimiento del brazo para lanzar una pelota virtual.
	\item RecorderDemo, ejemplo que muestra como grabar y reproducir un movimiento captado por kinect.
	\item SpeechRecognitionDemo, ejemplo que sirve para realizar acciones por comandos por voz, aunque se producen errores cuando se probó este ejemplo.
	\item VariousDemos, implementa dos ejemplos, como pintar en el aire moviendo los brazos y el otro dibuja bolas verdes que se colocan en tus articulaciones simulado un esqueleto.
	\item VisualizerDemo, este ejemplo convierte la escena, según lo ve el sensor, en una malla y la superpone sobre la imagen de la cámara.\\\\	
\end{itemize}
Una vez explicado los ejemplos, elegimos cual de ellos podríamos utilizar para aprovechar su funcionalidad y tener un apoyo base para el desarrollo del proyecto. Los ejemplos seleccionados serían el \texttt {AvatarsDemo, GestureDemo y RecorderDemo}, más adelante se explicar con mas detalle que se utiliza de estos ejemplos.\\\\

\subsection{Grabar y reproducir movimientos de Usuario}

\label{cap5:sec:Grabar y reproducir movimientos de Usuario}
Como el objetivo principal de este proyecto es el entrenamiento de actividades físicas se selecciona como ejemplo de primer estudio el \texttt{RecorderDemo} por su potencial para guardar y reproducir un movimiento.

\subsubsection{Investigación base}

En una primer vista nos muestra como representa al usuario captado por la kinect, esta representación se hace mediante un cubeman\footnote{Esqueleto verde que emula el movimiento de la persona captada.} que simula todo el movimiento que realiza el usuario.

Esta representación de \textit{cubeman} en el escenario de Unity es creada gracias al script de \texttt{Cubeman Controller}. Este script se inicializa con el número del cuerpo que quieres que muestre, kinect v2 puede detectar hasta 6 persona, y también si se quiere que el cuerpo este representado en modo espejo. Los datos del cuerpo del usuario se los a pide al \texttt{Kinect Manager} (este script se explica más adelante). Los datos del cuerpo están definidos con 25 GameObjects llamados \texttt{Joints} que hacen una representacion de la articulaciones del cuerpo humano en unas coordenadas (x,y,z) y su rotación.
Los 25 \texttt{Joints} serían : cadera central y laterales , pecho, clavícula, cuello, cabeza, hombros, codos, muñecas, pulgares, manos centrales, rodillas, tobillos y pies.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{Imagenes/Capitulos/capitulo5/Avatar-controller-joints}
	\caption{Cubeman con la lista de todos los Joints}
	\label{fig:avatar-controller-joints}
\end{figure}


Para hacer una representación correcta del esqueleto y se mueva como una entidad, el script \texttt{Cubeman Controller} pone la posición del Joint de la cadera base como la poscion y rotación del objeto padre y todos los demás joint serán hijos de este GameObject. Para calcular la posión relativa  de todos los Joints se resta la posición del padre con la posición de cada Joint dada por el Kinect Manager.

El script \texttt{Kinect Manager} es un Singletone\footnote{Es un patrón que consiste en que existe una única instancia y es la propia clase la responsable de crear la única instancia. Permite el acceso global a dicha instancia mediante un método de clase} encargado de la comunicación entre el sensor de kinect y las aplicaciones de Unity.
\begin{figure}[th!]
	\centering
	\includegraphics[width=0.7\linewidth]{Imagenes/Capitulos/capitulo5/singleton}
	\caption{Patrón de Singleton}
	\label{fig:singleton}
\end{figure}

Este script al ser común a todos los ejemplos nombrados anteriormente implementa toda la funciones y comunicaciones necesarias para su funcionamiento, pero en nuestro caso se investiga como obtiene la información que le envía al \texttt{cubeman Controller}.
Kinect Manager se encarga de analizar la información del \texttt{SensorData}, este SensorData es una estructura de datos ofrecida por el script \texttt{KinectInterop}\footnote{Script encargado de tener comunicación directa con el sensor de la kinect} donde aparece la información de la imagen, pronfundidad, color y datos de los usuarios captado con el sensor. Los datos de los usuarios captados se encapsulan en el \texttt{BodyDataFrame}, esta estructura contiene la información del número de usuarios captados, su identificador ,la información de los BodyData\footnote{Información de los joint de un cuerpo especifico}, etc.\\

Una vez comprendido el SensorDatar se observa como el Kinect Manager ,analizando el BodyDataFrame, va actualizando y asignando a cada usuario el Id correspodiente 
de un  BodyData en todo momento y pasando al Cubeman Controller esta información para que reproduzca el movimiento.\\

Otro script que se debe mencionar es el \texttt{KinectRecorderPlayer}, que es el encargado de guardar y reproducir un movimiento a partir de un fichero txt. La información se le pide  al \texttt{Kinect Manager} y este a su vez al \texttt{KinectInterop} devolviéndola en una cadena de caracteres. Esta cadena hace referencia a un frame\footnote{Es un fotograma, Unity por defecto  reproduce a 60 frames por segundo}  que tiene la información del instante de tiempo, los BodyData captados y las coordenadas (x,y,z) respecto al mundo de todos sus joinst. Los BodyData captados aparece primero su Id y después sus 25 joints, los BodyData no captados aparecerá un cero.
\texttt{KinecRecorderPlayer} también discierne entre grabación y reproducción activando solo una de la dos a la vez.

\comImpl{foto de txt}\\

\subsubsection{Cambios e implementaciones}

Después de haber realizado la investigación y compresión del material que se puede aprovechar para grabar y reproducir un movimiento se nos plantean una series de desafios e implementaciones que debemos realizar.

En primer lugar debemos implementar la forma de poder reproducir un movimiento grabado a la vez que muestra al usuario captado en tiempo real. Para ello implementamos una función llamada \texttt{SetBodyFrameFromCsvAndPoll} en el \texttt{KinectInterop} ,para así obtener, primero el SensorData con los datos del usuario captados por la kinect y después se busca un BodyData que esté libre de ese mismo SensorData para rellenarlo con los datos del fichero txt. También se añade un campo más a la estructura de BodyData para saber si los datos son procedentes del sensor o de un fichero. 
Posteriormente de hacer estos cambios, se realiza una modificación en el script \texttt{CubemanController} para poder escoger de que usuario se quiere mostrar el movimientos si proceden del sensor o de un fichero.
Con todas estas mejoras implementadas, se empieza a modificar el escenario de Unity para mostrar dos cámaras. La primera cámara enfoca un escenario donde se encuentra el cubeman que representa al usuario captado por el sensor de Kinect y la segunda cámara muestra otro escenario colocado en la esquina superior derecha que contiene otro cubeman que está a la espera de reproducir el movimiento que está guardado en un fichero.


\comImpl{foto esqueletos verdes con las dos camaras}\\
\subsection{Comparación de los movimientos de Usuario}

Otra etapa en el desarrollo de este proyecto es el comparar un movimiento grabado con el movimiento que este realizando el usuario.

\subsubsection{Investigación base} 

La primera forma de comparar un movimiento en la que se debatió fue la de estudiar las trayectorias de cada movimiento, calculando la gráfica que generaban las coordenadas de los joints a lo largo del tiempo y determinar la pendiente. Esta forma de comparación era muy especifica y laboriosa para cada movimiento, por lo que se intento tener otro camino para hacer la comparación de forma más genérica. Con esta idea en mente nos fijamos en el ejemplo de \texttt{GestureDemo} que reconoce gestos del usuario para rotar y hacer zoom sobre un cubo. Para que reconozca estos gestos previamente hay que añadirlos a las lista de gestos del script \texttt{KinectGesture} e implementar su funcionalidad en el método \textit{CheckForGesture}. Posteriormente hay que notificar al \textit{KinectManger} que gesto de los registrados queremos que se detecte.
En cuanto a la funcionalidad del gesto, se realiza por estados y progresión permaneciendo en el primer estado hasta que identifique una posición especifica. Para completar el gesto se tendrá un tiempo determinado para llegar a la progresión final del movimiento sino el gesto se cancelará.


\subsubsection{Cambios e implementaciones}

Una vez analizado la funcionalidad de los gestos de kinect nos apoyaremos en esa idea para implemenentar la comparación del movimiento como un gesto de kinect. Para empezar añadimos el gesto al \texttt{KinectGesture} con el nombre \textit{Move} e implementamos la funcionalidad en \textit{CheckForGesture} haciendo que tenga dos estados. El estado cero se encarga de calibrar la posición inicial dando al usuario la oportunidad de colocarse en esa posición y ofreciéndole un margen de tres segundos para que se prepare. 

Para el siguiente estado hay que explicar antes que la información del movimiento esta guardada previamente en una lista de \textit{strings} y cada elemento corresponde a un \textit{frame} , esta información será accedida en el estado.  
Después de este inciso se define el siguiente estado como el estado por defecto ,y es así porque engloba todos los siguiente estados al estado cero. El número del estado será utilizado para acceder y obtener el \textit{frame} de la lista del movimiento. En este momento decidimos que para sea más eficiente el salto al siguiente estado no incrementaría en uno sino en más unidades, ya que, la comparación de dos \textit{frames} consecutivos es muy similar y así se ahorrarían comparaciones innecesarias. Para pasar al siguiente estado habrá una comparación de cada uno de los  25 \textit{joints} del usuario con los \textit{joints} del \textit{frame} correspondiente. La comparación consiste en igualar las coordenadas x e y con un margen de error modificable por el usuario, si cumple este requisito pasará al siguiente estado. La comparación tiene un tiempo límite que sería el tiempo del movimiento más dos segundos de espera, en ese tiempo el usuario deberá superar todos los estados para que el movimiento sea realizado correctamente, en caso contrario fracasará y se cancelará el gesto. El resultado del gesto se muestra por pantalla para dar un \textit{feedback} inmediato al usuario. 

Como \textit{kinect} es una cámara frontal surgen algunos problemas de profundidad haciendo que la comparación de la coordenada z nunca se cumpla, por ello, se toma la decisión de obviala por su comportamiento extraño.  \\


\comImpl{Grafo de comparar movimiento}

\subsection{Avatares y animaciones}

En esta etapa del proyecto, se desarrollarán los  diferentes avatares con una fisionomía humana para dar una sensación mas realista a los \textit{cubeman} y de este modo se proporcionará una perspectiva mas amena al usuario.

\subsubsection{Investigación base} 

\comImpl{Foto de la configuración de rig en unity}

La primera aplicación que se utilizó para crear los personajes que darían vida al proyecto fue \texttt{MakeHuman}, ya que mostraba una interfaz amigable e intuitiva (ver Figura \ref{fig:makehuman_avatar} ). El primer avatar utilizado fue simple, ya que al principio interesaba ver el resultado que proporcionaba el personaje junto con el \textit{cubeman} y de esta forma determinar la configuración de los huesos que mas se ajustaba a las necesidades dadas. Para comprobar que esqueleto se debía emplear,se tuvieron que crear cuatro avatares diferentes, con 31, 163, 137 (ver Figura \ref{fig:makehuman_avatar_huesos3} )  y 53 huesos respectivamente.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar}
	\caption{Avatar inicial}
	\label{fig:makehuman_avatar}
\end{figure}

\vspace{1cm}

Después de realizar varias pruebas, se determinó que el avatar que mejor se adaptaba a los movimientos capturados por \textit{Kinect} era \textit{Default no toes} ver Figura \ref{fig:makehuman_avatar_huesos3} ). Este personaje poseía 137 huesos, de los cuales 25 serían utilizados por el \textit{cubeman}.

\begin{figure}[p!]
	\centering
	\includegraphics[width=1\linewidth, height=0.37\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar_huesos3}
	\caption{Avatar con 137 huesos}
	\label{fig:makehuman_avatar_huesos3}
\end{figure}


Para comprobar como se visualizaban los movimientos de un avatar con ropa, se utilizó el personaje inicial vestido con un peto y una camiseta (ver Figura \ref{fig:makehuman_avatar2} ). Al observar de los movimientos capturados con \texttt{Kinect} que las texturas de la piel y la ropa no plasmaban un realismo razonable, se optó por buscar otro software que ayudase a realizar esa tarea.


\begin{figure}[p!]
	\centering
	\includegraphics[width=1\linewidth, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar2}
	\caption{Avatar inicial con ropa}
	\label{fig:makehuman_avatar2}
\end{figure}

En la búsqueda se encontraron aplicaciones dedicadas al modelado de personajes 3D como \texttt{Blender}, pero con una curva de aprendizaje demasiado larga para las necesidades dadas. También se examino una aplicación llamada \texttt{MarvelousDesigner} la cual se emplea para desarrollar prendas de vestir. El problema surgía cuando se intentaba importar el avatar creado con \texttt{MakeHuman}, ya que no eran compatibles los formatos de las dos aplicaciones y por lo tanto se descartó seguir por esa vía. En el camino se observó que existía una compañía llamada \texttt{Mixamo} que ofrecía una aplicación para el desarrollo de avatares y a su vez, un entorno dedicado a la animación de los personajes creados. Por este motivo, se tomó la decisión de cambiar a \texttt{Fuse Character Creator de Mixamo} como aplicación para el desarrollo de los personajes, ya que se amoldaba perfectamente a las necesidades de este proyecto.

\subsection{Cambios e implementaciones}

A fin de realizar un entorno mas inmersivo se crearon cuatro avatares diferentes, cada uno con una fisionomía propia. A la hora de elaborar el vestuario de los personajes, se observó la ropa que utilizaban los integrantes de la escuela \texttt{Abadá-Capoeira} para tener un modelo a seguir. Se trataba de un pantalón largo y una camiseta de tirantes en color blanco con el logo de la escuela. 

Con el propósito de crear una ropa muy similar a la utilizada por la escuela  \texttt{Abadá-Capoeira}, se usaron prendas prediseñadas en la aplicación \texttt{Fuse Character Creator} como los pantalones, camisetas y el top que utilizarían los avatares del sistema con los retoques apropiados para darle ese color blanco característico.

Los dos avatares principales, como son el alumno y el profesor se desarrollaron con la misma vestimenta que utilizaba la escuela \texttt{Abadá-Capoeira}, en cambio los dos avatares que se emplean para dar un mayor dinamismo a la aplicación utilizan el mismo pantalón largo, mientras que el avatar masculino no utilizará prenda superior y el avatar femenino vestirá un top en color blanco.

Tras completar la estética de los personajes, \texttt{Fuse Character Creator de Mixamo} ofrece la posibilidad de configurar los huesos de los avatares, para ello es necesario subir a los servidores de \texttt{Mixamo} dichos personajes. Al finalizar la subida la aplicación redirigirá su actividad al navegador web, como ocurría con \texttt{MakeHuman}, \texttt{Mixamo} ofrece la posibilidad de crear varios esqueletos para un mismo esqueleto, en este caso se presentan cuatro casos con 25, 41 (ver Figura \ref{fig:mixamo-rig} ) , 49 y 65 huesos respectivamente.

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.7\linewidth, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/mixamo-rig}
	\caption{Auto-Rig de Mixamo}
	\label{fig:mixamo-rig}
\end{figure}

Una vez comprendida la generación de avatares. El avatar elegido es el que contiene 41 huesos porque demuestra una movilidad mas semejante al \textit{cubeman} inicial.

Con el propósito de utilizar las animaciones que suministra \texttt{Mixamo} se observó el ejemplo \texttt{AvatarsDemo}, el cual utiliza un script \texttt{AvatarController} que realiza una conexión entre los \textit{joints} y los huesos incorporados en el avatar. 

Posteriormente para que los avatares no se queden en una posición estática, se incorporaron las animaciones de la ginga, victoria y derrota para obtener un feedback mas ilustrado de la comparación de movimiento.
\comImpl{tres fotos ginga victoria y derrota}

Para incorporar estas animaciones al avatar, se crea el componente \texttt{Animator Controller} de \textit{Unity} encargado de añadir y gestionar animaciones. Este componente contiene una máquina de estados capaz de coordinar que animación se ejecuta en cada momento. El estado inicial será la ginga y realizará una transición a la animación de victoria o derrota que dependerá de la comparación del movimiento.  Una vez determinado el anterior estado, este volverá a la animación de ginga.\\

\comImpl{imagen Grafo animator}\\

Se constató que existía un conflicto entre el \texttt{Animator Controller} y el sensor de movimiento por quien tiene el control del avatar, por ello, se implementó un script que desactiva el componente \texttt{Animator Controller} cuando el sensor de \textit{kinect} detecta al usuario y se vuelve a activar cuando el usuario se sale de la escena.


\subsection{Análisis de los movimientos de Usuario}

Una parte importante de este proyecto consiste en dar una corrección del movimiento que esta imitando el usuario en tiempo real. En esta sección se explicará como se implementó este análisis.

\subsubsection{Investigación base}

Para este apartado se observó el desarrollo de la comparación de movimiento implementada anteriormente. Esa sección respondía al usuario dándole un \textit{feedback} inmediato si había realizado el movimiento correctamente o incorrecta, sin embargo , no le detallaba que partes del cuerpo había colocado de manera errónea. 

El planteamiento para el análisis comienza reproduciendo el movimiento del avatar del usuario junto al avatar de la grabación, deteniéndose en el \textit{frame} donde el usuario cometió el error. En ese instante cambiará los huesos del avatar controlado por el usuario a un color rojo, en consecuencia de una colocación incorrecta del hueso. Adicionalmente mostrará un panel con un texto describiendo las correcciones que se deben aplicar. Esta información se compondrá de parte de cuerpo, dirección y altura que servirá de pautas para que el usuario corrija su posición.

\subsubsection{Cambios e implementaciones}
Una vez planteada las especificaciones del análisis se empieza con  su implementación. En primer lugar se creó un script llamado \texttt{Registrar\_movimientos}, el cual, guarda toda la información del movimientos hasta donde el usuario comete el error. Esta información contiene el estado\footnote{Corresponde al frame que se esta comparando.} donde se produce el error y los datos de los \textit{joints} de los avatares.
Posteriormente se necesita hacer una modificación en \texttt{KinectGesture} en la parte de la funcionalidad del gesto \textit{Move}, para poder rellenar esa información. Esta modificación consiste en enviar la información del estado y los \textit{joints} de los avatares al \texttt{Registrar\_movimientos}, una vez superada la comparación de ese \textit{frame}. Con esta lógica se quedará registrado el movimiento hasta que el usuario cometa el error.

A partir de este momento, para poder mostrar la información recogida se implementa una funcionalidad nueva en el script \texttt{Registrar\_movimientos}, que consiste en desplazar los avatares a primera escena, modificando los materiales, para que se vean transparentes y generar un esqueleto verde parecido a los \textit{cubeman} del principio. Estos avatares simularán el movimiento hasta el \textit{frame} donde se equivocó el usuario y realizará una comprobación de cada \textit{joint} para cambiar el hueso de color de verde a rojo. A la vez, se va generando un \textit{string} con una corrección de posicionamiento compuesto por tres partes. Para la primera parte se crea un diccionario que,a partir del número del \textit{joint}, devuelve la parte del cuerpo correspondiente. En la implementación de este diccionario se han agrupado \textit{joints}, ya que varios forman una misma parte del cuerpo, como por ejemplo, las manos que lo forman tres \textit{joints} diferentes. La segunda parte del \textit{string} esta formada por la elección de la dirección , que  se comprueba, realizando la diferencia de la coordenadas X del \textit{joint} del usuario con la del \textit{joint} de la grabación. Si la diferencia es positiva  la dirección será hacia derecha, en caso contrario, será hacia la izquierda. Y para la tercera parte del \textit{string} se realiza la diferencia con la coordenada Y definiendo así la altura. Si la diferencia es positiva la altura sera hacia abajo , en caso contrario hacia arriba.
Esta lista de correcciones se mostrará en un panel una vez completada evitando frases repetidas.



\comImpl{Imagen completa de analisis con panel}


\subsection{Creación de los escenarios 3D}
\subsection{Grabacion de movimientos con gente experta}


%-------------------------------------------------------------------
%-------------------------------------------------------------------

