
%---------------------------------------------------------------------
%
%                          Capítulo 5
%
%---------------------------------------------------------------------

\chapter{Desarrollo Del Proyecto}



\label{cap5:sec:Desarrollo Del Proyecto}
Una vez elegida la tecnología y las plataformas que se van a utilizar, se empieza a preparar el desarrollo del proyecto. En primer lugar se investiga que librerías y paquetes se necesitan para tener una buena conexión con \texttt{Kinect} y, además, que información obtenida del sensor de movimiento podemos aprovechar para el proyecto.


%-------------------------------------------------------------------
\section{Paquetes para Unity}

\label{cap5:sec:Paquetes para Unity}
El primer paquete que se prueba es el que nos ofrece \texttt{Microsoft} \footnote{https://developer.microsoft.com/es-es/windows/kinect/tools},que es un paquete destinado principalmente para \texttt{UnityPro}.\footnote{Unity profesional de pago con mas funcionalidad que Unity personal.} Al añadir este primer paquete al proyecto se empieza a manifestar una serie de errores de scripts, esto es debido, a la incongruencia de versiones de Unity. El proyecto se desarrolla en Unity Personal, que es una versión gratuita de Unity , mientras que el paquete que ofrece Microsoft esta especificado para Unity Pro. Aun así, se corrigen errores de comandos y llamadas a funciones obsoletas para ver si se puede aprovechar este paquete.

En una primera instancia se ejecuta la escena que viene como ejemplo en el paquete de Unity para ver su funcionamiento y se observa que tiene un comportamiento intermitente, es decir, cuando nos colocamos enfrente de la cámara de kinect a veces mostraba un esqueleto verde que emulaba la persona captada y otras veces dejaba de funcionar sin saber el error producido.
Debatiendo e intentando comprender si estos errores se producían por conexión de kinect o por funcionamiento incorrecto de la librería que nos ofrecía microsoft se opto por la opción de descartar este paquete para evitar futuras fustraciones.

Después de descartar el paquete que nos ofrecía Microsoft decidimos buscar en el Assets Store de Unity(Explicacion o referencia).Encontramos el paquete \texttt {Kinect v2 Examples with MS-SDK}\textcolor{red}{(?Referencia o imagen?)} que lo elegimos por su valoración positiva y también porque hay poca variedad de paquetes relacionados con kinect v2.

Este paquete tiene todo lo necesario para reconocer que la kinect está conectada y para poder utilizar los datos que se obtienen del sensor. En una primera toma, el paquete nos ofrece una serie de ejemplos sencillos para poder comprender mejor el funcionamiento de kinect. Para que este paquete funcione es necesario tener instalado Kinect SDK 2.0 que son los drivers de la kinect v2 .


Los ejemplos que tiene implementado el paquete de Kinect v2 Examples with MS-SDK  son variados:
\begin{itemize}
	\item AvatarsDemo, simulador que muestra un avatar en tercera persona que correspondería a la persona captada y se puede controlar sobre el escenario 3D.
	\item BackgroundRemovalDemo, son ejemplos que cambian el fondo que se encuentra detrás del usuario captado.
	\item ColliderDemo, una serie de ejemplos para ver el funcionamiento de colisiones del usuario captado con los objetos que aparecen en la escena.
	\item FaceTrackingDemo, este ejemplo reconoce la dirección de tu cabeza para girar la cámara de la imagen para simular la vista humana.
	\item FittingRoomDemo, este ejemplo te da la opción de ponerte ropa encima de tu imagen real captada.
	\item GesturesDemo, serie de ejemplos de funcionamiento de los gestos de kinect.
	\item InteractionDemo, este ejemplo muestra como el usuario puede girar,rotar y agrandar un objeto con el movimiento de sus manos.
	\item KinectDataServer, implementa un servidor de datos para guardar información como gestos de kinect.
	\item MovieSequenceDemo, este ejemplo mostrará como reproducir un conjunto de frames de película con el cuerpo del usuario.
	\item MultiSceneDemo, este ejemplo concatenará diferentes escenas de Unity basadas en las componentes de este paquete.
	\item OverlayDemo, son tres ejemplos que muestras como interactuar con los objetos de la escena, para ello, se basa en el movimientos de los brazos y manos para hacer que los objetos se mueven, roten y se desplazen.
	\item PhysicsDemo, muestra un simulación de físicas que capta el movimiento del brazo para lanzar una pelota virtual.
	\item RecorderDemo, ejemplo que muestra como grabar y reproducir un movimiento captado por kinect.
	\item SpeechRecognitionDemo, ejemplo que sirve para realizar acciones por comandos por voz, aunque se producen errores cuando se probó este ejemplo.
	\item VariousDemos, implementa dos ejemplos, como pintar en el aire moviendo los brazos y el otro dibuja bolas verdes que se colocan en tus articulaciones simulado un esqueleto.
	\item VisualizerDemo, este ejemplo convierte la escena, según lo ve el sensor, en una malla y la superpone sobre la imagen de la cámara.\\\\	
\end{itemize}
Una vez explicado los ejemplos, elegimos cual de ellos podríamos utilizar para aprovechar su funcionalidad y tener un apoyo base para el desarrollo del proyecto. Los ejemplos seleccionados serían el \texttt {AvatarsDemo, GestureDemo y RecorderDemo}, más adelante se explicar con mas detalle que se utiliza de estos ejemplos.\\\\

\subsection{Grabar y reproducir movimientos de Usuario}

\label{cap5:sec:Grabar y reproducir movimientos de Usuario}

Como el objetivo principal de este proyecto es el entrenamiento de actividades físicas, se selecciona como ejemplo de primer estudio, el \texttt{Recorderdemo} por su potencial para guardar y reproducir un movimiento.


\subsubsection{Investigación base}

En una primera vista se muestra como \texttt{Kinect} capta al usuario, esta representación se hace mediante un cubeman\footnote{Esqueleto verde que emula el movimiento de la persona captada.} que simula todo el movimiento que realiza el usuario.

Esta representación de \textit{cubeman} en el escenario de \texttt{Unity}, es creada gracias al \textit{script} de \texttt{Cubeman Controller}. Este \textit{script} se inicializa con el número del cuerpo que se quiere mostrar, siendo 6 las personas que pueden ser detectadas por \texttt{\texttt{Kinect} v2}, y también pudiendo escoger si el \textit{cubeman} es representado en modo espejo. Los datos del cuerpo están definidos con 25 \texttt{GameObjects} llamados \texttt{Joints}, que hacen una representación de las articulaciones del cuerpo humano. Estos \texttt{Joints} se representan en \texttt{Unity} con unas coordenadas (X,Y,Z) y una rotación.
Los 25 \texttt{Joints} serían : cadera central y laterales , pecho, clavícula, cuello, cabeza, hombros, codos, muñecas, pulgares, manos centrales, rodillas, tobillos y pies.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{Imagenes/Capitulos/capitulo5/Avatar-controller-joints}
	\caption{Cubeman con la lista de todos los Joints}
	\label{fig:avatar-controller-joints}
\end{figure}


Para hacer una representación correcta del esqueleto y se mueva como una entidad, el \textit{script} \texttt{Cubeman Controller}, pone la posición del \texttt{Joint} de la cadera base como la posición y rotación del objeto padre y todos los demás joint serán hijos de este \texttt{GameObjects}. Para calcular la posición relativa  de todos los \texttt{Joints} se resta la posición del padre con la posición de cada \texttt{Joint} dada por el \texttt{\texttt{Kinect} Manager}.

El \textit{script} \texttt{\texttt{Kinect} Manager} es un \texttt{Singleton}\footnote{Es un patrón que consiste en que existe una única instancia y es la propia clase la responsable de crear la única instancia. Permite el acceso global a dicha instancia mediante un método de clase} encargado de la comunicación entre el sensor de \texttt{Kinect} y las aplicaciones de Unity.

\begin{figure}[th!]
	\centering
	\includegraphics[width=0.7\linewidth]{Imagenes/Capitulos/capitulo5/singleton}
	\caption{Patrón de Singleton}
	\label{fig:singleton}
\end{figure}

Este \textit{script} al ser común a todos los ejemplos nombrados anteriormente, implementa todas las funciones y comunicaciones necesarias para su funcionamiento, pero en este caso, se investiga como obtiene la información que le envía al \texttt{cubeman Controller}.
\texttt{\texttt{Kinect} Manager} se encarga de analizar la información del \texttt{SensorData}, siendo este, una estructura de datos ofrecida por el \textit{script} \texttt{\texttt{Kinect}Interop}\footnote{\textit{script} encargado de tener comunicación directa con el sensor de la \texttt{Kinect}} donde aparece la información de la imagen, pronfundidad, color y datos de los usuarios captado con el sensor. Los datos de los usuarios captados se encapsulan en el \texttt{BodyDataFrame}, esta estructura, a vez, contiene la información del número de usuarios captados, su identificador, la información de los BodyData\footnote{Información de los joint de un cuerpo especifico}, etc.\\

Una vez comprendido el \texttt{SensorDatar}, se observa como el \texttt{\texttt{Kinect} Manager} ,analizando el \texttt{BodyDataFrame}, va actualizando y asignando a cada usuario el identificador correspondiente
de un  \texttt{BodyData} en todo momento ,y este a su vez, va pasando al \texttt{Cubeman Controller} esta información para que reproduzca el movimiento.\\

Otro \textit{script} que se debe mencionar es el \texttt{\texttt{Kinect}RecorderPlayer}, que es el encargado de guardar y reproducir un movimiento a partir de un fichero de texto. La información se le pide  al \texttt{\texttt{Kinect} Manager}, y este a su vez, al \texttt{\texttt{Kinect}Interop} devolviéndola en una cadena de caracteres. Esta cadena hace referencia a un \textit{frame}\footnote{Es un fotograma, \texttt{Unity} por defecto reproduce a 60 \textit{frames} por segundo } que tiene la información del instante de tiempo, los \textit{BodyData} captados y las coordenadas (x,y,z) respecto al mundo de todos sus \texttt{Joints}. Los \textit{BodyData} captados aparece primero su identificador y después sus 25 \texttt{Joints}, los \textit{BodyData} no captados aparecerá un cero.
\texttt{KinecRecorderPlayer} también discierne entre grabación y reproducción activando solo una de la dos a la vez.

\comImpl{foto de txt}\\

\subsubsection{Cambios e implementaciones}

Después de haber realizado la investigación y compresión del material que se puede aprovechar para grabar y reproducir un movimiento, se plantean una series de desafíos e implementaciones que se deben realizar.

En primer lugar se debe implementar la forma de poder reproducir un movimiento grabado a la vez que muestra al usuario captado en tiempo real, para ello, se implementa una función llamada \texttt{SetBodyFrameFromCsvAndPoll} en el \texttt{\texttt{Kinect}Interop}. El funcionamiento de esta función es el de obtener primero el \texttt{SensorData} con los datos del usuario captados por la \texttt{Kinect} y después, se busca un \textit{BodyData} que esté libre de ese mismo SensorData, para así, poder rellenarlo con los datos del fichero de texto. También se añade un campo más a la estructura de \textit{BodyData} para saber si los datos son procedentes del sensor o de un fichero de texto. 
Posteriormente de hacer estos cambios, se realiza una modificación en el \textit{script} \texttt{Cubeman Controller} para poder escoger si los datos proceden del sensor o de un fichero texto.
Con todas estas mejoras implementadas, se empieza a modificar el escenario de \texttt{Unity} para mostrar dos cámaras. La primera cámara enfoca un escenario donde se encuentra el \textit{cubeman} que representa al usuario captado por el sensor de \texttt{Kinect} y la segunda cámara muestra otro escenario colocado en la esquina superior derecha que contiene otro \textit{cubeman} que está a la espera de reproducir el movimiento que está guardado en un fichero de texto.


\comImpl{foto esqueletos verdes con las dos camaras}\\
\subsection{Comparación de los movimientos de Usuario}

Otra etapa en el desarrollo de este proyecto se trata de comparar un movimiento grabado con el que esté realizando el usuario.

\subsubsection{Investigación base} 

La primera forma de comparar un movimiento en la que se debatió, fue la de estudiar las trayectorias de cada movimiento, calculando así, la gráfica que generaban las coordenadas de los \texttt{Joints} a lo largo del tiempo y determinar la pendiente. Esta forma de comparación era muy especifica y laboriosa para cada movimiento, por lo que se intento tener otro camino para hacer la comparación de forma más genérica.

Con esta idea en mente ,se estudió el ejemplo de \texttt{GestureDemo} que reconoce gestos del usuario para rotar y hacer \textit{zoom} sobre un cubo. Para que reconozca estos gestos previamente, hay que añadirlos a las lista de gestos del \textit{script} \texttt{\texttt{Kinect}Gesture} e implementar su funcionalidad en el método \textit{CheckForGesture}. Posteriormente hay que notificar al \texttt{\texttt{Kinect}Manager} que gesto de los registrados se quiere detectar.

En cuanto a la funcionalidad del gesto, se realiza por estados y progresión,  permaneciendo en el primer estado hasta que identifique una posición especifica. Para completar el gesto se tendrá un tiempo determinado para llegar a la progresión final del movimiento, sino el gesto se cancelará.


\subsubsection{Cambios e implementaciones}

Una vez analizado la funcionalidad de los gestos de \texttt{Kinect}, se apoyó en esa idea para implemenentar la comparación del movimiento como un gesto de \texttt{Kinect}. Para empezar añadimos el gesto al \texttt{\texttt{Kinect}Gesture} con el nombre \textit{Move}, acto seguido, se implementa la funcionalidad en \textit{CheckForGesture} haciendo que tenga dos estados. El estado cero se encarga de calibrar la posición inicial ,dando al usuario, la oportunidad de colocarse en esa posición y ofreciéndole un margen de tres segundos para que se prepare. 

Para el siguiente estado hay que explicar antes, que la información del movimiento esta guardada previamente en una lista de \textit{strings} y cada elemento corresponde a un \textit{frame}. 
Después de este inciso se define el siguiente estado como el estado por defecto ,y es así,  porque engloba todos los siguiente estados al estado cero. El número del estado será utilizado para acceder y obtener el \textit{frame} de la lista del movimiento. En este momento se decide que para que sea más eficiente la transisción de estados, se incrementa en más de una unidad el estado, ya que la comparación de dos \textit{frames} consecutivos es muy similar, ahorrandose así comparaciones innecesarias. Para pasar al siguiente estado habrá una comparación de cada uno de los  25 \texttt{Joints} del usuario con los \texttt{Joints} del \textit{frame} correspondiente. La comparación consiste en igualar las coordenadas x e y con un margen de error modificable por el usuario, si cumple este requisito pasará al siguiente estado. La comparación tiene un tiempo límite que será el tiempo del movimiento más dos segundos de espera, en ese tiempo el usuario deberá superar todos los estados para que el movimiento sea realizado correctamente, en caso contrario fracasará y se cancelará el gesto. El resultado del gesto se muestra por pantalla para dar un \textit{feedback} inmediato al usuario. 

Como \texttt{Kinect} es una cámara frontal surgen algunos problemas de profundidad haciendo que la comparación de la coordenada z se cumpla ocasionalmente, por ello, se toma la decisión de obviarla por su comportamiento extraño.\\

\comImpl{Grafo de comparar movimiento}

\subsection{Avatares y animaciones}

En esta etapa del proyecto, se desarrollarán los  diferentes avatares con una fisionomía humana para dar una sensación mas realista a los \textit{cubeman} y de este modo se proporcionará una perspectiva mas amena al usuario.

\subsubsection{Investigación base} 

\comImpl{Foto de la configuración de rig en unity}

La primera aplicación que se utilizó para crear los personajes que darían vida al proyecto fue \texttt{MakeHuman}, ya que mostraba una interfaz amigable e intuitiva (ver Figura \ref{fig:makehuman_avatar} ). El primer avatar utilizado fue simple, ya que al principio interesaba ver el resultado que proporcionaba el personaje junto con el \textit{cubeman} y de esta forma determinar la configuración de los huesos que mas se ajustaba a las necesidades dadas. Para comprobar que esqueleto se debía emplear,se tuvieron que crear cuatro avatares diferentes, con 31, 163, 137 (ver Figura \ref{fig:makehuman_avatar_huesos3} )  y 53 huesos respectivamente.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar}
	\caption{Avatar inicial}
	\label{fig:makehuman_avatar}
\end{figure}

\vspace{1cm}

Después de realizar varias pruebas, se determinó que el avatar que mejor se adaptaba a los movimientos capturados por \textit{Kinect} era \textit{Default no toes} ver Figura \ref{fig:makehuman_avatar_huesos3} ). Este personaje poseía 137 huesos, de los cuales 25 serían utilizados por el \textit{cubeman}.

\begin{figure}[p!]
	\centering
	\includegraphics[width=1\linewidth, height=0.37\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar_huesos3}
	\caption{Avatar con 137 huesos}
	\label{fig:makehuman_avatar_huesos3}
\end{figure}


Para comprobar como se visualizaban los movimientos de un avatar con ropa, se utilizó el personaje inicial vestido con un peto y una camiseta (ver Figura \ref{fig:makehuman_avatar2} ). Al observar de los movimientos capturados con \texttt{Kinect} que las texturas de la piel y la ropa no plasmaban un realismo razonable, se optó por buscar otro software que ayudase a realizar esa tarea.


\begin{figure}[p!]
	\centering
	\includegraphics[width=1\linewidth, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar2}
	\caption{Avatar inicial con ropa}
	\label{fig:makehuman_avatar2}
\end{figure}

En la búsqueda se encontraron aplicaciones dedicadas al modelado de personajes 3D como \texttt{Blender}, pero con una curva de aprendizaje demasiado larga para las necesidades dadas. También se examino una aplicación llamada \texttt{MarvelousDesigner} la cual se emplea para desarrollar prendas de vestir. El problema surgía cuando se intentaba importar el avatar creado con \texttt{MakeHuman}, ya que no eran compatibles los formatos de las dos aplicaciones y por lo tanto se descartó seguir por esa vía. En el camino se observó que existía una compañía llamada \texttt{Mixamo} que ofrecía una aplicación para el desarrollo de avatares y a su vez, un entorno dedicado a la animación de los personajes creados. Por este motivo, se tomó la decisión de cambiar a \texttt{Fuse Character Creator de Mixamo} como aplicación para el desarrollo de los personajes, ya que se amoldaba perfectamente a las necesidades de este proyecto.

\subsection{Cambios e implementaciones}

A fin de realizar un entorno mas inmersivo se crearon cuatro avatares diferentes, cada uno con una fisionomía propia. A la hora de elaborar el vestuario de los personajes, se observó la ropa que utilizaban los integrantes de la escuela \texttt{Abadá-Capoeira} para tener un modelo a seguir. Se trataba de un pantalón largo y una camiseta de tirantes en color blanco con el logo de la escuela. 

Con el propósito de crear una ropa muy similar a la utilizada por la escuela  \texttt{Abadá-Capoeira}, se usaron prendas prediseñadas en la aplicación \texttt{Fuse Character Creator} como los pantalones, camisetas y el top que utilizarían los avatares del sistema con los retoques apropiados para darle ese color blanco característico.

Los dos avatares principales, como son el alumno y el profesor se desarrollaron con la misma vestimenta que utilizaba la escuela \texttt{Abadá-Capoeira}, en cambio los dos avatares que se emplean para dar un mayor dinamismo a la aplicación utilizan el mismo pantalón largo, mientras que el avatar masculino no utilizará prenda superior y el avatar femenino vestirá un top en color blanco.

Tras completar la estética de los personajes, \texttt{Fuse Character Creator de Mixamo} ofrece la posibilidad de configurar los huesos de los avatares, para ello es necesario subir a los servidores de \texttt{Mixamo} dichos personajes. Al finalizar la subida la aplicación redirigirá su actividad al navegador web, como ocurría con \texttt{MakeHuman}, \texttt{Mixamo} ofrece la posibilidad de crear varios esqueletos para un mismo esqueleto, en este caso se presentan cuatro casos con 25, 41 (ver Figura \ref{fig:mixamo-rig} ) , 49 y 65 huesos respectivamente.

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.7\linewidth, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/mixamo-rig}
	\caption{Auto-Rig de Mixamo}
	\label{fig:mixamo-rig}
\end{figure}

Una vez comprendida la generación de avatares. El avatar elegido es el que contiene 41 huesos porque demuestra una movilidad mas semejante al \textit{cubeman} inicial.

Con el propósito de utilizar las animaciones que suministra \texttt{Mixamo} se observó el ejemplo \texttt{AvatarsDemo}, el cual utiliza un script \texttt{AvatarController} que realiza una conexión entre los \textit{joints} y los huesos incorporados en el avatar. 

Posteriormente para que los avatares no se queden en una posición estática, se incorporaron las animaciones de la ginga, victoria y derrota para obtener un feedback mas ilustrado de la comparación de movimiento.
\comImpl{tres fotos ginga victoria y derrota}

Para incorporar estas animaciones al avatar, se crea el componente \texttt{Animator Controller} de \textit{Unity} encargado de añadir y gestionar animaciones. Este componente contiene una máquina de estados capaz de coordinar que animación se ejecuta en cada momento. El estado inicial será la ginga y realizará una transición a la animación de victoria o derrota que dependerá de la comparación del movimiento.  Una vez determinado el anterior estado, este volverá a la animación de ginga.\\

\comImpl{imagen Grafo animator}\\

Se constató que existía un conflicto entre el \texttt{Animator Controller} y el sensor de movimiento por quien tiene el control del avatar, por ello, se implementó un script que desactiva el componente \texttt{Animator Controller} cuando el sensor de \textit{kinect} detecta al usuario y se vuelve a activar cuando el usuario se sale de la escena.


\subsection{Análisis de los movimientos de Usuario}

Una parte importante de este proyecto consiste en dar una corrección del movimiento, el cual, el usuario esta imitando en tiempo real. En esta sección se explicará como se implementó este análisis.

\subsubsection{Investigación base}

Para este apartado se observó el desarrollo de la comparación de movimiento implementada anteriormente. Esa sección respondía al usuario dándole un \textit{feedback} inmediato si había realizado el movimiento correctamente o incorrecta, sin embargo , no le detallaba que partes del cuerpo había colocado de manera errónea. 

El planteamiento del análisis comienza reproduciendo el movimiento del avatar del usuario junto al avatar de la grabación, deteniéndose ,si llegara el caso,  en el \textit{frame} donde el usuario cometió el error. En ese instante cambiará los huesos del avatar controlado por el usuario a un color rojo, en consecuencia de una colocación incorrecta del hueso. Adicionalmente mostrará un panel con un texto describiendo las correcciones que se deben aplicar. Esta información se compondrá de la parte de cuerpo que se está analizando , añadiendo seguidamente, la dirección y la altura que servirán de pautas para que el usuario corrija su posición.

\subsubsection{Cambios e implementaciones}
Una vez planteada las especificaciones del análisis se empieza con  su implementación. En primer lugar se creó un \textit{script} llamado \texttt{Registrar\_movimientos}, el cual, guarda toda la información del movimiento hasta donde el usuario comete el error. Esta información contiene el estado\footnote{Corresponde al frame que se esta comparando.} donde se produce el error y los datos de los \texttt{Joints} de los avatares.
Posteriormente se necesita hacer una modificación funcional en el gesto \textit{Move} de \texttt{\texttt{Kinect}Gesture}, para conseguir así, rellenar esa información. Esta modificación consiste en enviar la información del estado y los \textit{joints} de los avatares al \texttt{Registrar\_movimientos}, una vez superada la comparación de ese \textit{frame}. Con esta lógica se quedará registrado el movimiento hasta que el usuario cometa el error.

A partir de este momento, para poder mostrar la información recogida se implementó una funcionalidad nueva en el \textit{script} \texttt{Registrar\_movimientos}, que consiste en desplazar los avatares a primera escena, modificando los materiales para que se vean transparentes ,y crear así un esqueleto verde parecido a los \textit{cubeman} del principio. Estos avatares simularán el movimiento hasta el \textit{frame} donde se equivocó el usuario y realizará una comprobación de cada \texttt{Joint} para cambiar el hueso de color de verde a rojo. A la vez, se va generando un \textit{string} con una corrección de posicionamiento compuesto por tres partes. Para la primera parte se crea un diccionario que,a partir del número del \texttt{Joint}, devuelve la parte del cuerpo correspondiente. En la implementación de este diccionario se han agrupado \texttt{Joints}, ya que varios forman una misma parte del cuerpo, como por ejemplo, las manos que lo forman tres \texttt{Joints} diferentes. La segunda parte del \textit{string} esta formada por la elección de la dirección , que  se comprueba, realizando la diferencia de la coordenadas X del \texttt{Joint} del usuario con la del \texttt{Joint} de la grabación. Si la diferencia es positiva la dirección será hacia la derecha, en caso contrario, será hacia la izquierda. Y para la tercera parte del \textit{string} se realiza la diferencia con la coordenada Y definiendo así la altura. Si la diferencia es positiva la altura será hacia abajo , en caso contrario hacia arriba.
Esta lista de correcciones se mostrará en un panel una vez completada evitando los \textit{strings} repetidos.

\comImpl{Imagen completa de analisis con panel}


\subsection{Creación de los escenarios 3D}

Para el desarrollo de los escenarios, se ha intentado crear dos ambientes diferentes. Uno escenificando un entorno de entranamiento como es el caso de un gimnasio para la visualización del alumno y otro mas orientado al profesor y los animadores, como se trata de la playa de Brasil.

\subsection{Grabacion de movimientos con gente experta}


%-------------------------------------------------------------------
%-------------------------------------------------------------------

