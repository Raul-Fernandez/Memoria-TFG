
%---------------------------------------------------------------------
%
%                          Capítulo 5
%
%---------------------------------------------------------------------

\chapter{Desarrollo Del Proyecto}



\label{cap5:sec:Desarrollo Del Proyecto}
Una vez elegida la tecnología y plataformas que se van a utilizar se empieza preparar el desarrollo del proyecto. En primer lugar se investigan que librerías y paquetes se necesitan para tener una buena conexión con kinect y que información obtenida del sensor de movimiento podemos aprovechar para el proyecto.


%-------------------------------------------------------------------
\section{Paquetes para Unity}

\label{cap5:sec:Paquetes para Unity}
El primer paquete que se prueba es el que nos ofrece microsoft \footnote{https://developer.microsoft.com/es-es/windows/kinect/tools},que es un paquete destinado principalmente para UnityPro.\footnote{Unity profesional de pago con mas funcionalidad que Unity personal.} Al añadir este primer paquete al proyecto se empieza a manifestar una serie de errores de scripts, esto es debido, a la incongruencia de versiones de Unity. El proyecto se desarrolla en Unity Personal, que es una versión gratuita de Unity , mientras que el paquete que ofrece microsoft esta especificado para Unity Pro. Aun así, se corrigen errores de comandos y llamadas a funciones obsoletas para ver si se puede aprovechar este paquete o no.

En una primera instancia se ejecuta la escena que viene como ejemplo en el paquete de Unity para ver su funcionamiento y se observa que tiene un comportamiento intermitente, es decir, cuando nos colocamos enfrente de la cámara de kinect a veces mostraba un esqueleto verde que emulaba la persona captada y otras veces dejaba de funcionar sin saber el error producido.
Debatiendo e intentando comprender si estos errores se producían por conexión de kinect o por funcionamiento incorrecto de la librería que nos ofrecía microsoft se opto por la opción de descartar este paquete para evitar futuras fustraciones.

Después de descartar el paquete que nos ofrecía microsoft decidimos buscar en el Assets Store de Unity(Explicacion o referencia).Encontramos el paquete \texttt {Kinect v2 Examples with MS-SDK}\textcolor{red}{(?Referencia o imagen?)} que lo elegimos por su valoración positiva y también porque hay poca variedad de paquetes relacionados con kinect v2.

Este paquete tiene todo lo necesario para reconocer que la kinect está conectada y para poder utilizar los datos que se obtienen del sensor. En una primera toma, el paquete nos ofrece una serie de ejemplos sencillos para poder comprender mejor el funcionamiento de kinect. Para que este paquete funcione es necesario tener instalado Kinect SDK 2.0 que son los drivers de la kinect v2 .


Los ejemplos que tiene implementado el paquete de Kinect v2 Examples with MS-SDK  son variados:
\begin{itemize}
	\item AvatarsDemo, simulador que muestra un avatar en tercera persona que correspondería a la persona captada y se puede controlar sobre el escenario 3D.
	\item BackgroundRemovalDemo, son ejemplos que cambian el fondo que se encuentra detrás del usuario captado.
	\item ColliderDemo, una serie de ejemplos para ver el funcionamiento de colisiones del usuario captado con los objetos que aparecen en la escena.
	\item FaceTrackingDemo, este ejemplo reconoce la dirección de tu cabeza para girar la cámara de la imagen para simular la vista humana.
	\item FittingRoomDemo, este ejemplo te da la opción de ponerte ropa encima de tu imagen real captada.
	\item GesturesDemo, serie de ejemplos de funcionamiento de los gestos de kinect.
	\item InteractionDemo, este ejemplo muestra como el usuario puede girar,rotar y agrandar un objeto con el movimiento de sus manos.
	\item KinectDataServer, implementa un servidor de datos para guardar información como gestos de kinect.
	\item MovieSequenceDemo, este ejemplo mostrará como reproducir un conjunto de frames de película con el cuerpo del usuario.
	\item MultiSceneDemo, este ejemplo concatenará diferentes escenas de Unity basadas en las componentes de este paquete.
	\item OverlayDemo, son tres ejemplos que muestras como interactuar con los objetos de la escena, para ello, se basa en el movimientos de los brazos y manos para hacer que los objetos se mueven, roten y se desplazen.
	\item PhysicsDemo, muestra un simulación de físicas que capta el movimiento del brazo para lanzar una pelota virtual.
	\item RecorderDemo, ejemplo que muestra como grabar y reproducir un movimiento captado por kinect.
	\item SpeechRecognitionDemo, ejemplo que sirve para realizar acciones por comandos por voz, aunque se producen errores cuando se probó este ejemplo.
	\item VariousDemos, implementa dos ejemplos, como pintar en el aire moviendo los brazos y el otro dibuja bolas verdes que se colocan en tus articulaciones simulado un esqueleto.
	\item VisualizerDemo, este ejemplo convierte la escena, según lo ve el sensor, en una malla y la superpone sobre la imagen de la cámara.\\\\	
\end{itemize}
Una vez explicado los ejemplos, elegimos cual de ellos podríamos utilizar para aprovechar su funcionalidad y tener un apoyo base para el desarrollo del proyecto. Los ejemplos seleccionados serían el \texttt {AvatarsDemo, GestureDemo y RecorderDemo}, más adelante se explicar con mas detalle que se utiliza de estos ejemplos.\\\\

\subsection{Grabar y reproducir movimientos de Usuario}

\label{cap5:sec:Grabar y reproducir movimientos de Usuario}
Como el objetivo principal de este proyecto es el entrenamiento de actividades físicas se selecciona como ejemplo de primer estudio el \texttt{Recordermo} por su potencial para guardar y reproducir un movimiento.
\section*{Investigación base}
En una primer vista nos muestra como representa al usuario captado por la kinect, esta representación se hace mediante un cubeman\footnote{Esqueleto verde que emula el movimiento de la persona captada.} que simula todo el movimiento que realiza el usuario.




Esta representación de cubeman en el escenario de Unity es creada gracias al script de \texttt{Cubeman Controller}. Este script se inicializa con el número del cuerpo que quieres que muestre, kinect v2 puede detectar hasta 6 persona, y también si se quiere que el cuerpo este representado en modo espejo. Los datos del cuerpo del usuario se los a pide al \texttt{Kinect Manager} (este script se explica más adelante). Los datos del cuerpo están definidos con 25 GameObjects llamados \texttt{Joints} que hacen una representacion de la articulaciones del cuerpo humano en unas coordenadas (x,y,z) y su rotación.
Los 25 \texttt{Joints} serían : cadera central y laterales , pecho, clavícula, cuello, cabeza, hombros, codos, muñecas, pulgares, manos centrales, rodillas, tobillos y pies.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{Imagenes/Capitulos/capitulo5/Avatar-controller-joints}
	\caption{Cubeman con la lista de todos los Joints}
	\label{fig:avatar-controller-joints}
\end{figure}


Para hacer una representación correcta del esqueleto y se mueva como una entidad, el script \texttt{Cubeman Controller} pone la posición del Joint de la cadera base como la poscion y rotación del objeto padre y todos los demás joint serán hijos de este GameObject. Para calcular la posión relativa  de todos los Joints se resta la posición del padre con la posición de cada Joint dada por el Kinect Manager.

El script \texttt{Kinect Manager} es un Singletone\footnote{Es un patrón que consiste en que existe una única instancia y es la propia clase la responsable de crear la única instancia. Permite el acceso global a dicha instancia mediante un método de clase} encargado de la comunicación entre el sensor de kinect y las aplicaciones de Unity.
\begin{figure}[th!]
	\centering
	\includegraphics[width=0.7\linewidth]{Imagenes/Capitulos/capitulo5/singleton}
	\caption{Patrón de Singleton}
	\label{fig:singleton}
\end{figure}

Este script al ser común a todos los ejemplos nombrados anteriormente implementa toda la funciones y comunicaciones necesarias para su funcionamiento, pero en nuestro caso se investiga como obtiene la información que le envía al \texttt{cubeman Controller}.
Kinect Manager se encarga de analizar la información del \texttt{SensorData}, este SensorData es una estructura de datos ofrecida por el script \texttt{KinectInterop}\footnote{Script encargado de tener comunicación directa con el sensor de la kinect} donde aparece la información de la imagen, pronfundidad, color y datos de los usuarios captado con el sensor. Los datos de los usuarios captados se encapsulan en el \texttt{BodyDataFrame}, esta estructura contiene la información del número de usuarios captados, su identificador ,la información de los BodyData\footnote{Información de los joint de un cuerpo especifico}, etc.\\

Una vez comprendido el SensorDatar se observa como el Kinect Manager ,analizando el BodyDataFrame, va actualizando y asignando a cada usuario el Id correspodiente 
de un  BodyData en todo momento y pasando al Cubeman Controller esta información para que reproduzca el movimiento.\\

Otro script que se debe mencionar es el \texttt{KinectRecorderPlayer}, que es el encargado de guardar y reproducir un movimiento a partir de un fichero txt. La información se le pide  al \texttt{Kinect Manager} y este a su vez al \texttt{KinectInterop} devolviéndola en una cadena de caracteres. Esta cadena hace referencia a un frame\footnote{Es un fotograma, Unity por defecto  reproduce a 60 frames por segundo}  que tiene la información del instante de tiempo, los BodyData captados y las coordenadas (x,y,z) respecto al mundo de todos sus joinst. Los BodyData captados aparece primero su Id y después sus 25 joints, los BodyData no captados aparecerá un cero.
\texttt{KinecRecorderPlayer} también discierne entre grabación y reproducción activando solo una de la dos a la vez.

\comImpl{foto de txt}\\

\section*{Cambios e implementaciones}

Después de haber realizado la investigación y compresión del material que se puede aprovechar para grabar y reproducir un movimiento se nos plantean una series de desafios e implementaciones que debemos realizar.

En primer lugar debemos implementar la forma de poder reproducir un movimiento grabado a la vez que muestra al usuario captado en tiempo real. Para ello implementamos una función llamada \texttt{SetBodyFrameFromCsvAndPoll} en el \texttt{KinectInterop} ,para así obtener, primero el SensorData con los datos del usuario captados por la kinect y después se busca un BodyData que esté libre de ese mismo SensorData para rellenarlo con los datos del fichero txt. También se añade un campo más a la estructura de BodyData para saber si los datos son procedentes del sensor o de un fichero. 
Posteriormente de hacer estos cambios, se realiza una modificación en el script \texttt{CubemanController} para poder escoger de que usuario se quiere mostrar el movimientos si proceden del sensor o de un fichero.
Con todas estas mejoras implementadas, se empieza a modificar el escenario de Unity para mostrar dos cámaras. La primera cámara enfoca un escenario donde se encuentra el cubeman que representa al usuario captado por el sensor de Kinect y la segunda cámara muestra otro escenario colocado en la esquina superior derecha que contiene otro cubeman que está la espera de reproducir el movimiento que está guardado en un fichero.


\comImpl{foto esqueletos verdes con las dos camaras}\\
\subsection{Comparación de los movimientos de Usuario}
Otra etapa en el desarrollo de este proyecto es el comparar un movimiento grabado con el movimiento que este realizando el usuario.
\section*{Investigación base} 
La primera forma de comparar un movimiento en la que se debatió fue la de estudiar las trayectorias de cada movimiento, calculando la gráfica que generaban las coordenadas de los joints a lo largo del tiempo y determinar la pendiente. Esta forma de comparación era muy especifica y laboriosa para cada movimiento, por lo que se intento tener otro camino para hacer la comparación de forma más genérica. Con esta idea en mente nos fijamos en el ejemplo de \texttt{GestureDemo} que reconoce gestos del usuario para rotar y hacer zoom sobre un cubo. Para que reconozca estos gestos previamente hay que añadirlos a las lista de gestos del script \texttt{KinectGesture} e implementar su funcionalidad en el método \textit{CheckForGesture}. Posteriormente hay que notificar al \textit{KinectManger} que gesto de los registrados queremos que se detecte.
En cuanto a la funcionalidad del gesto, se realiza por estados y progresión permaneciendo en el primer estado hasta que identifique una posición especifica. Para completar el gesto se tendrá un tiempo determinado para llegar a la progresión final del movimiento sino el gesto se cancelará.


\section*{Cambios e implementaciones}
Una vez analizado la funcionalidad de los gestos de kinect nos apoyaremos en esa idea para implemenentar la comparación del movimiento como un gesto de kinect. Para empezar añadimos el gesto al \texttt{KinectGesture} con el nombre \textit{Move} e implementamos la funcionalidad en \textit{CheckForGesture} haciendo que tenga dos estados. El estado cero se encarga de calibrar la posición inicial dando al usuario la oportunidad de colocarse en esa posición y ofreciéndole un margen de tres segundos para que se prepare. 

Para el siguiente estado hay que explicar antes que la información del movimiento esta guardada previamente en una lista de \textit{strings} y cada elemento corresponde a un \textit{frame} , esta información será accedida en el estado.  
Después de este inciso se define el siguiente estado como el estado por defecto ,y es así porque engloba todos los siguiente estados al estado cero. El número del estado será utilizado para acceder y obtener el \textit{frame} de la lista del movimiento. En este momento decidimos que para sea más eficiente el salto al siguiente estado no incrementaría en uno sino en más unidades, ya que, la comparación de dos \textit{frames} consecutivos es muy similar y así se ahorrarían comparaciones innecesarias. Para pasar al siguiente estado habrá una comparación de cada uno de los  25 \textit{joints} del usuario con los \textit{joints} del \textit{frame} correspondiente. La comparación consiste en igualar las coordenadas x e y con un margen de error modificable por el usuario, si cumple este requisito pasará al siguiente estado. La comparación tiene un tiempo límite que sería el tiempo del movimiento más dos segundos de espera, en ese tiempo el usuario deberá superar todos los estados para que el movimiento sea realizado correctamente, en caso contrario fracasará y se cancelará el gesto. El resultado del gesto se muestra por pantalla para dar un \textit{feedback} inmediato al usuario. 

Por problemas de profundidad de \textit{kinect} se ha obviado la comparación de la coordenada z por su comportamiento extraño. \\


\comImpl{Grafo de comparar movimiento}

\subsection{Avatares y animaciones}

En esta etapa del proyecto, se desarrollarán los  diferentes avatares con una fisionomía humana para dar una sensación mas realista a los \textit{cubeman} y de este modo se proporcionará una perspectiva mas amena al usuario.

\comImpl{Foto de la configuración de rig en unity}

La primera aplicación que se utilizó para crear los personajes que darían vida al TFG fue MakeHuman, ya que mostraba una interfaz amigable e intuitiva. El primer avatar utilizado fue simple, ya que al principio interesaba ver el resultado que proporcionaba el personaje junto con el \textit{cubeman} y de esta forma determinar el numero de huesos utilizados que mas se ajustaba a las necesidades dadas. Para comprobar que esqueleto se debía utilizar,se tuvieron que crear cuatro avatares diferentes, con 31, 163, 137 y 53 huesos respectivamente. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth,, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar}
	\caption{Avatar inicial}
	\label{fig:makehuman_avatar}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth,, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar_huesos1}
	\caption{Avatar con 31 huesos}
	\label{fig:makehuman_avatar_huesos1}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth,, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar_huesos2}
	\caption{Avatar con 163 huesos}
	\label{fig:makehuman_avatar_huesos2}
\end{figure}

Después de realizar varias pruebas, se determinó que el avatar que mejor se adaptaba a los movimientos capturados por \textit{kinect} era \textit{Default no toes}. Este personaje poseía 137 huesos, de los cuales 25 serían utilizados por el \textit{cubeman}. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth,, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar_huesos3}
	\caption{Avatar con 137 huesos}
	\label{fig:makehuman_avatar_huesos3}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth,, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar_huesos4}
	\caption{Avatar con 53 huesos}
	\label{fig:makehuman_avatar_huesos4}
\end{figure}

Para comprobar como se visualizaban los movimientos de un avatar con ropa, se utilizó el personaje inicial vestido con un peto y una camiseta. Al observar de los movimientos capturados con \textit{kinect} que las texturas de la piel y la ropa no plasmaban un realismo razonable, se optó por buscar otro software que ayudase a realizar esa tarea.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth,, height=0.4\textheight]{Imagenes/Capitulos/capitulo5/makehuman_avatar2}
	\caption{Avatar inicial con ropa}
	\label{fig:makehuman_avatar2}
\end{figure}

Tras la decisión tomada de cambiar a Fuse Character Creator como aplicación para el desarrollo de los personajes

\section*{Investigación base} 

Para saber como animar un avatar nos fijamos en el ejemplo \texttt{AvatarsDemo}, para ello utiliza un script \texttt{AvatarController} que realiza una conexión entre los joints y los huesos incorporados en el avatar. RAUL TU SIGUES CON LOS PROGRAMAS Y ANIMACIONES.

\section*{Cambios e implementaciones}
Una vez comprendido la generación de avatares, se realizan una prueba con cuatro avatares que se diferencian en la cantidad de huesos que llevan incorporados. El avatar elegido es el que contiene 52 huesos porque demuestra una movilidad mas semejante al \textit{cubeman} inicial.\\
\comImpl{captura con los avatar integrando de makehuman con huesos en unity}

Posteriormente para que los avatares no se queden en una posición estática se incorporan la animaciones como el movimiento de la ginga y las animaciones de victoria y derrota para el feedback de la comparación de movimiento.\\
\comImpl{tres fotos ginga victoria y derrota}

Para incorporar estas animaciones se crea un \texttt{Animator Controller} que es un componente de Unity encargado de añadir y gestionar animaciones a un avatar. Este componente tiene una máquina de estados para gestionar que animación se ejecuta en cada momento. En nuestro caso el estado inicial será la animación de la ginga y realizara una transición a la animación de victoria o derrota si recibe un disparador de la comparación de movimiento y una vez terminada volverá a la animación de ginga.\\

\comImpl{imagen Grafo animator}\\
Hay que constatar que hay un conflicto entre el \texttt{Animator Controller} y el sensor de movimiento por quien tiene el control del avatar,por ello, se ha implementado un script que cuando el sensor de kinect detecta al usuario automáticamente se desactivar el componente \texttt{Animator Controller} y cuando el usuario se sale de escena el componente vuelve a activarse. 
\subsection{Analisis de los movimientos de Usuario}
\subsection{Creación de los escenarios 3D}


%-------------------------------------------------------------------
%-------------------------------------------------------------------

